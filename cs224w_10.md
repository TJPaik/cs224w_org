# Lecture 10

## 10.1 Heterogeneous Graphs and Relational GCNs
- Heterogeneous Graphs
	- graphs with multiple node and edge types
	- formal definintion
		- $G = (V,E,R,T)$
		- node types: $T(v)\in T$ defined for each $v\in V$
		- edges with relation types, or *triples*: $(v_i,r,v_j)\in E$ where $r\in R$
	- example: knowledge graphs
	- handled by relational GCNs

- Relational GCN
	- use differet weights for differet relation types
	- message passing
	$$h_{v}^{(l+1)} = \sigma(\sum_{r\in R}\sum_{u\in N_v^r}\frac{1}{|N_v^r|}W_r^{(l)}h_u^{(l)} + W_0^{(l)}h_v^{(l)})$$
  	- message: $m_{u,r}^{(l)}=\frac{1}{|N_v^r|}W_r^{(l)}h_u^{(l)}$, $m_{v}^{(l)}=W_0^{(l)}h_v^{(l)}$
	- aggregation: $h_{v}^{(l+1)} = \sigma(\textrm{Sum}(\{m_{u,r}^{(l)}:u\in N(v)\cup\{v\}\}))$

- Regularizing R-GCNs
	- number of parameters grows rapidly as number of relation grows
	- two regularization methods
    	1. Block Diagonal Matrics
			- use block diagonal matrices with $B$ blocks for $W$
			- reduces number of parameters to $1/B$
			- limitation: only nearby neurons can interact through $W$
    	2. Basis Learning
			- share weights across different relations by represeting each matrix as a linear combination of *basis* or *dictionanry* matrices $V_1,\cdots,V_B$
			- weight matrices
				$$W_r = \sum_{b=1}^B a_{rb}V_b$$

- Tasks
    - Node Classification
    - Link Prediction
        - split(message edge, supervision edge, validation and test edges) for each relation type then merge together
        - training
            - use R-GCN to score each triple $(v_i,r,v_j)$:
            $$f_{r}(h_{v_i},h_{v_j})=h_{v_i}^{T}W_rh_{v_j}$$
            - positive edges: training supervision edges
            - negative edges: perturb the tail of supervision edges
            - train with cross entropy loss
        - validation
            - calculate score of each validation edge and its perturbations not in training edge
            - rank the edges based on predicted scores and use following metrics
                - *Hits at k* (Hits@k): percentage of validation edges that score in the top $k$
                - *mean reciprocal rank* (MRR): mean of reciprocal of rankings of validation edges

## 10.2 Knowledge Graphs
- "Knowledge in graph form"
    - a type of heterogeneous graph which captures entities, types and relationships
    - nodes: real-world entities with types
    - edges: relationships between entities

- Common Characteristics
    - massive (millions of nodes)
    - incomplete (many edges missing)
        - e.g. in `FreeBase`, 75% of 3 million person entries have no nationality
        - common task for knowledge graphs is *knowledge graph completion*, which is to automatically infer incomplete information

## 10.3 Knowledge Graph Completion Algorithms
- KG Completion
    - for a given head and relation, need to predict missing tail
    - idea
        - represent entities and relations by shallow embeddings
        - score each triple using a score function $f(h,r,t)$
        - optimize embedding using negative sampling

- Score functions
    - represent entities $h,t$ by vectors $v_h,v_t\in \mathbb{R}^d$
    - `TransE`: $-||v_h+v_r-v_t||$ where $v_r\in \mathbb{R}^d$
    - `TransR`: $-||W_r v_h + v_r - W_r v_t||$ where $v_r\in \mathbb{R}^k$, $M_r\in\mathbb{R}^{r\times d}$
    - `DistMult`: $v_h^T W_r v_t$ where $W_r\in \mathbb{R}^{d\times d}$ is a diagonal matrix
    - `ComplEx`: $\textrm{Re}(c_h^T C_r \bar{c_t})$ where $c_h,c_t\in \mathbb{C}^d$, $C_r\in \mathbb{C}^{d\times d}$ is a diagonal matrix

- Modeling Relations: Examples
    - `TransE` cannot model symmetric or 1-to-N relations, while `TransR` can
    - `DistMult` cannot model antisymmetric relations, while `ComplEx` can