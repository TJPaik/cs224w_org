# Lecture 6
## Lecture 6.1: Recap & Difficulty
* Node Embeddings(ex. deep walk / node2vec)
    * Similar nodes in the graph are embedded closed together (i.e. $\operatorname{similarity}(u, v) \approx \mathbf{z}_v^{\mathrm{T}} \mathbf{z}_u$)
    * Two key components
        * Encoder  
        * Similarity function
    * Limitations
        * $O(|V|)$ parameters are needed:
        * Inherently "transductive" (we can only make predictions over the examples that we have actually seen during the training phase)
        * Do not incorporate node features
* Today: Deep Graph encoders
    * Tasks we wil be able to solve
        * Node classification
        * Link prediction
        * Community detection
        * Network similarity
* Why is it hard?
    * Networks are far more complex (than images or text)

----

## Lecture 6.2: Basic of deep learning
- Objective function:
$$\min_{\Theta} \mathcal{L}(\boldsymbol{y}, f(\boldsymbol{x}))$$
- $f$ can be a simple linear layer, an MLP, or other neural networks (e.g., a GNN later)
- Sample a minibatch of input $x$
- Forward propagation: compute $\mathcal{L}$ given $x$
- Back-propagation: obtain gradient $\nabla_{\Theta} \mathcal{L}$ using a chain rule
- Use stochastic gradient descent (SGD) to optimize for $\Theta$ over many iterations
----
## Lecture 6.3: Deep learning for Graphs

* Setup:  Graph with node features
* Graph Convolutional Networks
    * Idea: transform information at the neighbors and combine it:
        * Transform "messages" $h_i$ from neighbors: $W_i h_i$
        * (For example,) Add them up: $\sum_i W_i h_i$
    * Every node defines a computation graph based on its neighborhood!
    * For each layer embedding, we don't have notion of convergence.

* Deep Encoder - basic approach:

$$\mathrm{h}_v^{(l+1)}=\sigma\bigg(\mathrm{W}_l \sum_{u \in \mathrm{N}(v)}\frac{\mathrm{h}_u^{(l)}}{|\mathrm{N}(v)|}+\mathrm{B}_l \mathrm{h}_v^{(l)}\bigg), \forall l \in\{0, \ldots, L-1\}$$
* Matrix form: 
$$H^{(l+1)}=\sigma(\tilde{A} H^{(l)} W_l^{\mathrm{T}}+H^{(l)} B_l^{\mathrm{T}})$$
where $\tilde{A}=D^{-1} A$

### How to train a GNN ?
* Supervised setting
    * node wise label
* Unsupervised setting
    * use the graph structure as the supervision.
