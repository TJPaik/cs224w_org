{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 4: Graph as Matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 강의에서 배우는 것\n",
    "- random walk를 통해 node importance 결정(PageRank)\n",
    "- matrix factorization을 통한 node embedding\n",
    "- Node2Vec와 같은 다른 node embedding들도 matrix factorization의 관점에서 볼 수 있음\n",
    "\n",
    "random walk, matrix factorization, node embedding은 서로 연관이 있음.\n",
    "\n",
    "Link Analysis approaches to compute the importance of nodes in a graph:\n",
    "- PageRank\n",
    "- Personlized PageRank(PPR)\n",
    "- Random Walk with Restarts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 4.1: PageRank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Flow Model**\n",
    "\n",
    "주어진 graph의 node $j$의 importance $r_j$는 다음과 같이 나타낼 수 있다.\n",
    "$$\n",
    "r_j = \\sum_{i \\rightarrow j} \\frac{r_i}{d_i}\n",
    "$$\n",
    "여기서, $i \\rightarrow j$는 node $i$에서 node $j$로 in-link를 나타내고, $d_i$는 node $i$의 out-degree이다.\n",
    "\n",
    "또한, $i \\rightarrow j$일 때, $(j,i)$-성분이 $1/d_i$인 행렬 $M$을 생각하면, flow equation을 다음과 같이 쓸 수 있다. (이런 행렬 $M$을 stochastic adjacency matrix라고 한다.)\n",
    "$$\n",
    "r = Mr\n",
    "$$\n",
    "그리고 rank vector $r$은 principle eigenvector로 볼 수도 있고, 그래프 위에서 random walk의 stationary distribution으로도 볼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 4.2: PageRank: How to solve?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Power Iteration Method**\n",
    "\n",
    "위의 flow equation은 다음과 같은 power iteration을 통해서 풀 수 있다.\n",
    "- intialize $r^{(0)} = [1/N, \\cdots, 1/N]^\\top$\n",
    "- iterate $r^{(t+1)} = Mr^{(t)}$\n",
    "- stop when $|r^{(t+1)} - r^{(t)}|_1 < \\epsilon$\n",
    "\n",
    "그런데,\n",
    "- 위의 방법이 실제로 수렴할까?\n",
    "- 우리가 원하는 방향으로 수렴할까?\n",
    "- reasonable한 결과가 나올까?\n",
    "\n",
    "하는 의문이 남는다. 또한, 실제로 다음과 같은 경우에 문제가 발생하기도 한다.\n",
    "- out-link가 없는 경우(dead end)\n",
    "    - importance가 새 나감.\n",
    "- out-link가 자기 자신만을 가리키는 경우(spider trap)\n",
    "    - 결국 한쪽으로 모든 importance가 쏠림."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution to Spider Traps**\n",
    "\n",
    "매 time step마다 다음 두 가지 선택을 한다.\n",
    "- follow a link at random with probability $\\beta$.\n",
    "- jump to a random page with probability $1-\\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution to Dead Ends**\n",
    "\n",
    "dead end에서 임의의 node로 teleport한다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Google Matrix**\n",
    "\n",
    "The Google Matrix $G$:\n",
    "$$\n",
    "G = \\beta M + \\frac{(1-\\beta)}{N} \\mathbf{1} \\mathbf{1}^\\top\n",
    "$$\n",
    ", $[1/N]_{N \\times N}$ is an $N$ by $N$ matrix whose entries are all $1/N$. \n",
    "\n",
    "(그런데, 이 matrix는 dead end에 대한 solution은 들어가있지 않은 것 아닌지..?)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 4.3: Random Walk with Restarts and Personalized PageRank"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bipartite graph를 생각하자. (예를 들어, user와 item간의 interaction을 나타내는 graph) 이때, 어떤 item Q와 가장 연관이 있는 item은 무엇인지 어떻게 찾을 수 있을까?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Personalized PageRank: 기본적인 PageRank와는 다르게 전체 node의 부분집합으로만 teleport한다.\n",
    "- Random Walks with Restarts: starting node인 Q로만 teleport한다.\n",
    "\n",
    "위의 두 개 + 기본적인 PageRank가 서로 같은 꼴로 표현된다.\n",
    "$$\n",
    "G = \\beta M + (1-\\beta)\\ T\n",
    "$$\n",
    "여기서, $T$는 어떻게 teleport할지를 나타내는 행렬이다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 4.4: Matrix Factorization and Node Embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Z$를 embedding matrix(혹은 encoder)라고 하자. row의 개수는 embedding dimension이고, 각 column $Z_v$는 node $v$의 embedding vector가 된다. 만약 node $u$와 $v$가 '비슷'하다면, 이들의 내적 $Z_u^\\top Z_v$는 큰 값을 가져야한다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Simplest Node Similarity**\n",
    "\n",
    "Similarity를 가장 간단하게 정의하는 방법 중 하나는, 두 node가 edge로 연결되어 있으면 similar하다고 정의하는 것이다. 즉, $A$를 adjacency matrix라고 할 때, $Z^\\top_u Z_v = A_{u,v}$가 되게하는 (혹은 $Z^\\top Z = A$) embedding matrix $Z$를 찾는 것이다. 하지만 일반적으로 embedding dimension이 node의 개수보다 훨씬 작으므로, $A$가 $Z^\\top Z$와 같은 꼴로 딱 떨어지게 factorize되지 않는다. 따라서, 다음과 같은 minimization problem을 풀어서 $Z$를 approximate하게 찾는다.\n",
    "$$\n",
    "\\underset{Z}{\\mathrm{minimize}} \\left\\Vert A - Z^\\top Z \\right\\Vert_2.\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Walk-based Similarity**\n",
    "\n",
    "DeepWalk나 node2vec에서는 node similarity가 훨씬 복잡하게 정의되어 있는데, 이들 모두 위와 비슷하게 minimization problem을 푸는 것과 동치라는 것이 알려져있다.\n",
    "\n",
    "예를 들어, DeepWalk는 다음과 같은 행렬을 factorize하는 것과 동치이다.\n",
    "$$\n",
    "\\log \\left( \\mathrm{vol\\ }(G) \\left( \\frac{1}{T} \\sum_{r=1}^{T} \\left( D^{-1} A \\right)^r \\right) D^{-1} \\right) - \\log b\n",
    "$$\n",
    "($\\mathrm{vol\\ }(G) = \\sum_i \\sum_j A_{i,j}$, $T$는 length of random walk, $D$는 diagonal matrix s.t. $D_{u,u} = \\deg(u)$, b는 number fo negative samples.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Limitations**\n",
    "\n",
    "- Cannot obtain embeddings for nodes not in the training set.\n",
    "- Cannot capture structural similarity\n",
    "- Cannot utilize node, edge and graph features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c577d501962af2839a95c8d2261c0e458a9c2297d29af11cc57e8b4c666b6fb0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
