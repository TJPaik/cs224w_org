# Lecture 9
## Lecture 9.1: How Expressive ar Graph Neural Network
### How well can a GNN distinguish different graph structure?  
- Setting: The same features for each node
- Key concept: Computational graph
  - Most expressive GNN maps different rooted subtrees into different node embeddings (injective function)
- Key observation: Subtrees of the same depth can be `recursively` characterized from the leaf nodes to the root nodes.
  - If each step of GNN's aggregation can fully retain the neighboring information, the generated node embeddings can distinguish different rooted subtrees.
  - we only need to focus on single level of the tree.

---

## Lecture 9.2: Designing the Most Powerful GNNs
- Observation: Neighbor aggregation can be abstracted as a function over a multi-set (a set with repeating elements).
- Failure case
  - GCN(mean-pool) [Kipf and Welling]
    - a, b / a, a, b, b
  - GraphSAGE(max-pool) [Hamilton et al.]
    - a, b / a, a, b, b / a, a, b
### We need injective neighbor aggregation function over multisets.
Here, we design a neural network that can model injective multiset function.

---

`Theorm` (Xu et al. ICLR 2019): (Conditions: countable / boundedness). Any injective multi-set function can be expressed as:
$$\Phi\left(\sum_{\substack{x \in S}} f(x)\right).$$

---

- How to model $\Phi$ and $f$ in $\Phi\left(\sum_{x \in S} f(x)\right)$ ?
  - We use a Multi-Layer Perceptron (MLP). (Universal Approximation Theorem [Hornik et al., 1989])
    - 1-hidden-layer MLP with sufficiently-large hidden dimensionality and appropriate non-linearity $\sigma(\cdot)$ (including ReLU and sigmoid) can approximate any continuous function to an arbitrary accuracy.
  - We have arrived at a neural network that can model any injective multiset function.
    - Graph Isomorphism Network (GIN) [xu et al. ICLR 2019]
    $$
    \operatorname{MLP}_{\Phi}\left(\sum_{x \in S} \operatorname{MLP}_f(x)\right)
    $$
    - In practice, MLP hidden dimensionality of 100 to 500 is sufficient.
    - GIN is THE most expressive GNN in the class of message-passing GNNs.

### Relation to WL(Weisfeiler-Lehman) Graph Kernel
- We now describe the full model of GIN by relating it to WL graph kernel (traditional way of obtaining graph-level features).
  - WL kernel : $c^{(k+1)}(v)=\operatorname{HASH}\left(c^{(k)}(v),\left\{c^{(k)}(u)\right\}_{u \in N(v)}\right)$
  - We will see how GIN is a "neural network" version of the WL graph kernel.
    - GIN uses a neural network to model the injective HASH function.

--- 

`Theorem` (xu et al. ICLR 2019): Any injective function over the tuple can be modeled as 
  $$\operatorname{MLP}_{\Phi}\left((1+\epsilon) \cdot \operatorname{MLP}_f(c^{(k)}(v))+\sum_{u \in N(v)} \operatorname{MLP}_f(c^{(k)}(u))\right) $$ 
where $\epsilon$ is a learnable scalar.

---
Example: If input feature $c^{(0)}(v)$ is represented as onehot, direct summation is injective.

--- 
- Because of the relation between GIN and the WL graph kernel, their expressive is exactly the same.
  - If two graphs can be distinguished by GIN, they can be also distinguished by the WL kernel, and vice versa.
### Can expressive power of GNNs be improved?
- You can further improve the expressive power of graph neural networks.
  - So the important characteristic of what we talked today was that node features are indistinguishable, meaning that all nodes have the same node feature information.
  - So by adding rich features, nodes may become distinguishable.
