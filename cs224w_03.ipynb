{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 3: Graph Representation Learning\n",
    "We wanna kind of automatically learn the features, not using hand-crafted features.  \n",
    "Goal: task-independent feature learning for machine learning with graphs.\n",
    "\n",
    "---\n",
    "## Lecture 3.1: Node Embeddings: Encoder and Decoder\n",
    "* Encode nodes so that similarity in the embedding space(dot product / angle) approximates similarity in the original graph.  \n",
    "* Decoder maps from embeddings to the simliarity score.\n",
    "* Shallow encoding : Embedding matrix (#(nodes) x embedding dim)\n",
    "    * embedding-lookup\n",
    "\n",
    "How to define node similarity: Random walk based method in this course.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "## Lecture 3.2: Random Walk Approaches for Node Embeddings\n",
    "Notation:\n",
    "* $z_u$: embedding of node $u$\n",
    "* $P(v|z_u)$: The (predicted) probability of visiting node $v$ on random walks starting from node $u$.\n",
    "* $N_R(u)$: multiset of neighborhood of $u$ obtained by some random walk strategy $R$.\n",
    "* $\\sigma$: sigmoid function\n",
    "\n",
    "Goal: $z_u^T z_v\\sim$ probability that $u$ and $v$ co-occur on a random walk over the graph.  \n",
    "Idea: Learn node embedding such that `nearby` nodes are close together.\n",
    "\n",
    "* Learn $f:V\\rightarrow \\mathbb{R}^d$  \n",
    "* Objective: $\\max _f \\sum_{u \\in V} \\log \\mathrm{P}\\left(N_{\\mathrm{R}}(u) \\mid \\mathbf{z}_u\\right)$ \n",
    "* Equivalently, minimze $\\mathcal{L}=-\\sum_{u \\in V} \\sum_{v \\in N_R(u)}\\log \\left(P\\left(v \\mid \\mathbf{z}_u\\right)\\right)$\n",
    "* Parameterize the probability using softmax $P\\left(v \\mid \\mathbf{z}_u\\right)=\\frac{\\exp \\left(\\mathbf{z}_u^{\\mathrm{T}} \\mathbf{z}_v\\right)}{\\sum_{n \\in V} \\exp \\left(\\mathbf{z}_u^{\\mathrm{T}} \\mathbf{z}_n\\right)}$\n",
    "\n",
    "## Negative sampling\n",
    "$\\log \\left(\\frac{\\exp \\left(\\mathbf{z}_u^{\\mathrm{T}} \\mathbf{z}_v\\right)}{\\sum_{n \\in V} \\exp \\left(\\mathbf{z}_u^{\\mathrm{T}} \\mathbf{z}_n\\right)}\\right) \\approx \\log \\left(\\sigma\\left(\\mathbf{z}_u^{\\mathrm{T}} \\mathbf{z}_v\\right)\\right)-\\sum_{i=1}^k \\log \\left(\\sigma\\left(\\mathbf{z}_u^{\\mathrm{T}} \\mathbf{z}_{n_i}\\right)\\right), n_i \\sim P_V$\n",
    "\n",
    "Sample $k$ negative nodes each with prob. proportional to its degree\n",
    "Two considerations for $k$ (\\# negative samples):\n",
    "1. Higher $k$ gives more robust estimates\n",
    "2. Higher $k$ corresponds to higher bias on negative events\n",
    "In practive, $k=5\\sim 20$.\n",
    "\n",
    "## How should we randomly walk? (strategy R)\n",
    "* Node2vec: Biased random walks that can trade off between the local and global views(BFS vs DFS).\n",
    "    * BFS: Micro-vew of neighbourhood\n",
    "    * DFS: Macro-vew of neighbourhood\n",
    "    * Two parameters:\n",
    "        * return parameter $p$\n",
    "        * in-out parameter $q$\n",
    "            * Moving outwards (DFS) vs. inwards (BFS)\n",
    "            * Intuitively, $q$ is the \"ratio\" of BFS vs. DFS\n",
    "\n",
    "![image](src/cs224w_3.png)\n",
    "\n",
    "---\n",
    "## Lecture 3.3: Embedding Entire Graphs\n",
    "* Approach 1\n",
    "    * Run a standard graph embedding and sum or average the node embedding\n",
    "* Approach 2\n",
    "    * Introduce a \"virtual node\" to represent the (sub)graph and run a standard graph embedding technique\n",
    "* Approach 3: Anonymous Walk Embedding\n",
    "    * States in `anonymous` walks correspond to the index of the first time we visited the node in a random walk\n",
    "    * For each random walk, we get a sequence of numbers: 1/2/1/2/3/..\n",
    "    * Represent the graph as a prob. distrib. over these walks.\n",
    "    * For example:\n",
    "        * Set $l=3$\n",
    "        * Then we can represent the graph as a 5-dim vector - Since there are 5 anonymous walks $w_i$ of length 3: 111,112, 121, 122, 123\n",
    "        * $Z_G[i]=$ probability of anonymous walk $w_i$ in $G$\n",
    "    * How many random walks $m$ do we need?\n",
    "        * We want the distribution to have error of no more than $\\varepsilon$ with prob. less than $\\delta$ :\n",
    "        * $m=\\left[\\frac{2}{\\varepsilon^2}\\left(\\log \\left(2^\\eta-2\\right)-\\log (\\delta)\\right)\\right]$\n",
    "\n",
    "* Approach 4: Learn Walk Embedding\n",
    "    * Learn a graph embedding together with all the anonymous walk embedding $z_i$.\n",
    "        * Sample anonymous random walk from a node $\\{w_1, w_2, \\dots, w_i\\}$.\n",
    "        * Learn to predict walks that co-occur in $\\Delta$-size window (e.g. predict $w_2$ given $w_1, w_3$ if $\\Delta=1$ ) \n",
    "        * Objective:\n",
    "            $$\n",
    "            \\max \\sum_{t=\\Delta}^{T-\\Delta} \\log P\\left(w_t \\mid w_t-\\Delta, \\ldots, w_{t+\\Delta}, \\mathbf{z}_G\\right)\n",
    "            $$\n",
    "            * $P\\left(w_t \\mid\\left\\{w_{t-\\Delta}, \\ldots, w_{t+\\Delta}, z_G\\right\\}\\right)=\\frac{\\exp \\left(y\\left(w_t\\right)\\right)}{\\sum_{i=1}^\\eta \\exp \\left(y\\left(w_i\\right)\\right)}$\n",
    "            * $y\\left(w_t\\right)=b+U \\cdot\\left(\\operatorname{cat}\\left(\\frac{1}{2 \\Delta} \\sum_{i=-\\Delta}^{\\Delta} z_i, z_G\\right)\\right)$\n",
    "        * Sum the objective over all nodes in the graph\n",
    "        * After training, we can use the inner product btw graphs or use this vector as input to classify.\n",
    "\n",
    "---\n",
    "Preview: Hierachical Embeddings  \n",
    "\n",
    "We will discuss more advanced ways to obtain graph embeddings in Lecture 8.\n",
    "We can hierarchically cluster nodes in graphs, and sum/avg the node embeddings according to these clusters.\n",
    "* graph pooling\n",
    "---\n",
    "* How to use node embeddings?\n",
    "    * clustering / community detection\n",
    "    * node classification\n",
    "    * link prediction\n",
    "        * Concatenate\n",
    "        * Hadamard\n",
    "        * Sum/Avg\n",
    "        * Distance\n",
    "    * graph embedding\n",
    "        * via aggregating node embeddings or anonymous random walk"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
