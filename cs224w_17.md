# Lecture 17

## Lecture 17.1: Scaling up Graph Neural Networks to Large Graphs
We will introduce three methods for scaling up GNNs:
- Message-passing over small subgraphs in each mini-batch; only the subgraphs need to be loaded on a GPU at a time.
  - `Neighbor Sampling` [Hamilton et al. NeurPSS 2017]
  - `Cluster-GCN` [chiang et al. kDD 2019]
- One method simplifies a GNN into feature preprocessing operation (can be efficiently performed even on a CPU)
  - `Simplified GCN` [Wu et al. IcML 2019]


## Lecture 17.2: GraphSAGE Neighbor Sampling: Scaling up GNNs
- For a computational graph for each single node: 
  - exponentially large with respect to th layer size.
  - when the graph contains a hub node


### `Neighbor sampling` for $K$-layer GNN
- For each node in $k$-hop neighborhood, randomly sample at most $H_k$ neighbors.
  - Remark 1: Trade-off in sampling number $H$
    - Smaller $H$ leads to more efficient neighbor aggregation, but results in more unstable training due to the larger variance in neighbor aggregation.
  - Remark 2: Computational time
    - Even with neighbor sampling, the size of the computational graph is still exponential with respect to number of GNN layers $K$.
  - Remark 3: How to sample the nodes
    - highly skewed degree distributions
    - Random sampling: fast but many times not optimal (may sample many "unimportant" nodes)
    - `Random Walk` with Restarts:
      - Strategy to sample important nodes
      - This strategy works much better in practice.


## Lecture 17.3: Cluster GCN: Scaling up GNNs

- Issues with Neighbor Sampling
  - The size of computational graph becomes exponentially large w.r.t. the GNN layers.
  - Computation is redundant, especially when nodes in a mini-batch share many neighbors.
- `Cluster-GCN`:
  - Subgraph Sampling
    - Exploiting Community Structure(e.g. Louvain, METIS)
  - "vanilla" Cluster-GCN
    - partition $\rightarrow$ Mini-batch training
    - Issue: 
      - message from other groups will be lost
      - Similar nodes in the same group $\rightarrow$ Fluctuation
  - "Advanced" Cluster-GCN
    - Many relatively-small groups
    - Includes between-group edges


Comparison of Time Complexity
- $M$ nodes / $K$ layer
  - Neighbor-sampling
    -  sample $H$ $\rightarrow$ $MH^K$.
  - Cluster-GCN
    - $D_{avg}$: average node degree $\rightarrow$ $KMD_{avg}$.
    - much more efficient


## Lecture 17.4: Scaling up by Simplifying GNNs
### Removing the non-linear activation from the GCN
- Normalized adjacency matrix with self-loops:
$$
A \leftarrow A+I
$$
$$
\widetilde{A} \equiv D^{-1 / 2} \boldsymbol{A} D^{-1 / 2}
$$

$$
\boldsymbol{E}^{(k+1)}=\operatorname{ReLU}\left(\widetilde{\boldsymbol{A}} \boldsymbol{E}^{(k)} \boldsymbol{W}^{(k)}\right)\quad\rightarrow \quad \boldsymbol{E}^{(k+1)}=\widetilde{\boldsymbol{A}} \boldsymbol{E}^{(k)} \boldsymbol{W}^{(k)}
$$

* Issues:
  * Expressive power
  * Nodes connected by edges tend to have similar pre-processed features.
  * Many node classification tasks exhibit homophily structure, i.e., nodes connected by edges tend to share the same target labels.
  * Simplified GNNâ€™s prediction aligns well with the graph homophily in many node classification benchmark datasets.