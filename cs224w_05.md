# CS224W Lecture 5

## Message passing and Node Classification

- Task: *Semi-supervised node classification*
	- feature vector $f_v$ given for each node $v$
	- label(1 or 0) $Y_v$ given for some nodes $v$
	- predict $P(Y_v)$ for all nodes $v$

- Framework: *Message Passing*
	- Method
		- each node recieves messages(information such as features and labels) from its neighbors, and use them to update its own representation
		- this process is repeated iteratively until convergence or max number of iterations is reached
	- Intuition
		- correlations exist in networks, i.e. nearby nodes have similar characteristics
		- motivation from social sciences: *homophily* and *influence*
	- Remark
		- message passing is incorporated in GCNs, which are widely used in semi-supervised node classification
	
- Three classical techniques of message passing
	- *Relational classification*
	- *Iterative classification*
	- *Collective classification*

## Relational Classification

- *Probabilistic Relational Classifier*
	- Initialize labels
		- $Y_v = Y_v^*$ (ground truth) if labeled
		- $Y_v = 0.5$ if unlabeled
	- Update class probabilities
		- calculate weighted average of class probabilities of neighbors:
		$$P(Y_v = c) = \frac{1}{\sum_{(v,u)\in E} A_{v,u}} \sum_{(v,u)\in E} A_{v,u} P(Y_u=c)$$
	- Challenges
		- convergence not guaranteed
		- does not use node features

## Iterative Classification

- Approach
	- Train two classifiers
		- *base classifier* $\phi_1(f_v)$: predict label based on node feature $f_v$ (e.g. linear classifier)
		- *relational classifier* $\phi_2(f_v,z_v)$: predict label based on node feature $f_v$ and summary $z_v$ of labels of $v$'s neighbors
	- Examples of summary $z_v$
		- histogram of the number of each label in $N_v$
		- most common label in $N_v$
		- number of different labels in $N_v$

- Architecture
	- Phase 1: Train classifiers on training set
		- Train $\phi_1(f_v)$ to predict $Y_v$ based on $f_v$ (use these predictions to obtain summaries $z_v$)
		- Train $\phi_2(f_v,z_v)$ to predict $Y_v$ based on $f_v$ and $z_v$ 
	- Phase 2: Iterative classification
		- Update $z_v$ based on the new $Y_u$ for all $u\in N_v$
		- Update $Y_v$ based on the new $z_v$
		- Repeat until convergence or max number of iterations is reached

## Collective Classification

- *Loopy Belief Propagation*
	- an iterative process in which a probabilistic belief is passed between the nodes in a graph 
	- "loopy" since we consider general graphs with cycles, in which the message passing can  happen in a loopy manner

- Notation
	- *Label-label potential matrix* $\psi(Y_i,Y_j)$: probability of node $j$ being in class $Y_j$ given that it has a neighbor $i$ in class $Y_i$
	- *Prior belief* $\phi(Y_i)$: probability of node $i$ being in class $Y_i$
	- *message* $m_{i\to j}(Y_j)$: estimate of $j$ being in class $Y_j$
	- $\mathcal{L}$: set of all labels

- Algorithm
	1. Initialize all messages to $1$
	2. Repeat for each node $i$ (start from arbitrary node and follow along edges):
	$$m_{i\to j}(Y_j) = \sum_{Y_i\in\mathcal{L}}\psi(Y_i,Y_j)\phi(Y_i)\prod_{k\in N_i\setminus j}m_{k\to i}(Y_i)$$
	3. Belief after convergence:
	$$b_i(Y_i) = \phi(Y_i)\prod_{j\in N_i}m_{j\to i}(Y_i)$$